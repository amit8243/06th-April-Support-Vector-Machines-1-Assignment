{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526de637-6f6e-4977-81c4-b9907331ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines-1 Assignment\n",
    "\"\"\"Q1. What is the mathematical formula for a linear SVM?\"\"\"\n",
    "Ans: The mathematical formula for a linear SVM is: \n",
    "\n",
    "w^T x + b = 0, \n",
    "where w is the weight vector, x is the feature vector, and b is the bias.\n",
    "\n",
    "\"\"\"Q2. What is the objective function of a linear SVM?\"\"\"\n",
    "Ans: The objective function of a linear SVM is to maximize the margin between the two classes of data \n",
    "points. This is done by minimizing the sum of the squared distances from each data point to its \n",
    "respective class boundary.\n",
    "\n",
    "The objective function can be written as:\n",
    "\n",
    "minimize ùê∂‚àëùëñ=1ùëõ(ùë§‚ãÖùë•_i‚àíùëè)^2 + ùõº‚à•ùë§‚à•^2\n",
    "\n",
    "where C is a regularization parameter, ùë§ is the weight vector, ùë•_i is the ith data point, b is the bias\n",
    "term, and ùõº is a regularization parameter.\n",
    "\n",
    "\"\"\"Q3. What is the kernel trick in SVM?\"\"\"\n",
    "Ans: The kernel trick is a mathematical technique used in support vector machines (SVMs) to transform \n",
    "data into a higher-dimensional space, allowing for nonlinear classification. The kernel trick allows \n",
    "the SVM to learn complex, nonlinear decision boundaries without explicitly computing the coordinates of\n",
    "the data in a higher-dimensional space.\n",
    "\n",
    "In other words, the kernel trick is a way of using linear classifiers to solve nonlinear problems. By\n",
    "applying a kernel function to the data, the SVM can learn complex decision boundaries without having to \n",
    "explicitly compute the coordinates of the data in a higher-dimensional space.\n",
    "\n",
    "\"\"\"Q4. What is the role of support vectors in SVM Explain with example\"\"\"\n",
    "Ans: Support vectors are the data points that lie closest to the decision boundary in a Support Vector \n",
    "Machine (SVM). These points are the most important elements of an SVM model, as they determine the \n",
    "position and orientation of the decision boundary.\n",
    "\n",
    "For example, consider a binary classification problem with two classes, A and B. The data points are \n",
    "shown in the figure below. The support vectors are the four points that lie closest to the decision \n",
    "boundary (the dashed line). These points determine the position and orientation of the decision \n",
    "boundary, which separates class A from class B.\n",
    "\n",
    "\"\"\"Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\"\"\"\n",
    "Ans: Hyperplane:\n",
    "\n",
    "A hyperplane is a decision boundary that separates two classes of data points in a multi-dimensional \n",
    "space. It is represented by a line, plane, or higher-dimensional surface that divides the space into \n",
    "two distinct regions. In the case of a two-dimensional space, the hyperplane is a line that divides the\n",
    "space into two halves.\n",
    "\n",
    "\n",
    "Marginal Plane:\n",
    "\n",
    "A marginal plane is a hyperplane that is used to separate two classes of data points in a multi-\n",
    "dimensional space. It is represented by a line, plane, or higher-dimensional surface that divides the \n",
    "space into two distinct regions. The marginal plane is used to determine the optimal decision boundary \n",
    "between two classes of data points.\n",
    "\n",
    "\n",
    "Soft Margin:\n",
    "\n",
    "Soft margin is a technique used in support vector machines (SVMs) to allow for some misclassification \n",
    "of data points. It is used when the data points are not linearly separable, and it allows for some \n",
    "flexibility in the decision boundary. The soft margin technique allows for some misclassification of \n",
    "data points, while still maintaining a good overall accuracy.\n",
    "\n",
    "\n",
    "Hard Margin:\n",
    "\n",
    "Hard margin is a technique used in support vector machines (SVMs) to ensure that all data points are \n",
    "correctly classified. It is used when the data points are linearly separable, and it ensures that the \n",
    "decision boundary is as close to the data points as possible. The hard margin technique does not allow \n",
    "for any misclassification of data points, and it can lead to overfitting if the data is not linearly \n",
    "separable.\n",
    "\n",
    "\"\"\"Q6. Q6. SVM Implementation through Iris dataset.\n",
    "a) Load the iris dataset from the scikit-learn library and split it into a training set and a testing \n",
    "set\"\"\"\n",
    "\n",
    "Ans: \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "~ Implement the SVM classifier from scratch using the training set.\n",
    "\n",
    "# Define the SVM classifier\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize the weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # Compute the hinge loss\n",
    "            y_pred = self.hinge_loss(X)\n",
    "\n",
    "            # Compute the gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y)) + self.lambda_param * self.w\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # Update the weights and bias\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def hinge_loss(self, X):\n",
    "        return np.maximum(0, 1 - (X.dot(self.w) + self.b) * y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute the prediction\n",
    "        y_pred = np.sign(X.dot(self.w) + self.b)\n",
    "        return y_pred\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVM()\n",
    "\n",
    "# Fit the data to the classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "~ Compare the performance of the scikit-learn SVM classifier with the one implemented from scratch.\n",
    "\n",
    "# Make predictions using the scikit-learn SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Make predictions using the SVM classifier implemented from scratch\n",
    "y_pred_scratch = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy of scikit-learn SVM classifier:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Accuracy of SVM classifier implemented from scratch:\", accuracy_score(y_test, y_pred_scratch))\n",
    "\n",
    "\"\"\" b) Train a linear SVM classifier on the training set and predict the labels for the testing set\"\"\"\n",
    "\n",
    "Ans: Step 1: Import the necessary libraries\n",
    "\n",
    "import numpy as np \n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Step 2: Load the Iris dataset\n",
    "\n",
    "iris = datasets.load_iris() \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "Step 3: Split the dataset into training and testing sets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) \n",
    "\n",
    "Step 4: Train the SVM model \n",
    "\n",
    "from sklearn.svm import SVC \n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train) \n",
    "svm_predictions = svm_model_linear.predict(X_test) \n",
    "\n",
    "Step 5: Evaluate the model \n",
    "\n",
    "accuracy = svm_model_linear.score(X_test, y_test) \n",
    "cm = confusion_matrix(y_test, svm_predictions) \n",
    "\n",
    "Step 6: Print the results \n",
    "\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Confusion Matrix:\", cm)\n",
    "\n",
    "\"\"\"C) Compute the accuracy of the model on the testing set\"\"\"\n",
    "\n",
    "Ans: The accuracy of the SVM model on the Iris dataset can be computed using the sklearn library. The\n",
    "following code can be used to compute the accuracy of the model:\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "y_pred = svm_model.predict(X_test) \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\"\"\"D) Plot the decision boundaries of the trained model using two of the featuresl\"\"\"\n",
    "Ans: import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = (x_max / x_min)/100\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    " np.arange(y_min, y_max, h))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.title('SVC with linear kernel')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"E) Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\"\"\"\n",
    "Ans: # Step 1: Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 2: Importing the dataset\n",
    "dataset = pd.read_csv('Iris.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Step 3: Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Step 4: Training the SVM model on the Training set\n",
    "svc = SVC(kernel='linear', C=1.0)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Making predictions on the Test set\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'.format(accuracy))\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
